{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1.定义神经网络学习率以及权重\n",
    "2.遍历数据集的输入\n",
    "3.神经网络对数据进行处理\n",
    "4.计算对应的损失\n",
    "5.反向传播神经网络梯度的参数\n",
    "6.更新神经网络的权重\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        #Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
    "        #默认stride=(1,1),每次行的长度加上1，列的长度加上1\n",
    "        #1为输入的通道，6为输出的通道，3为卷积核的尺寸\n",
    "        #in_channel为1，outchannel为6，对应的卷积结果为3\n",
    "        #这里的in_channel就是之前的outchannel，如果是第一层\n",
    "        #的话就是开始的out_channel\n",
    "        \n",
    "        #这里的channel可以理解为图形的样本数量，\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        #第二个卷积核对应的第一个参数为6，为输入的通道\n",
    "        #第二个参数16为输出的通道，第三个参数3为卷积核的尺寸\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        #输入对应的第一层为576，由576转为120\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        #输入对应的第二层为由120转为84\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        #输入对应的第三层为由84转为10\n",
    "        \n",
    "        #!!!注意：这上面的操作只是定义了下面需要使用的参数，\n",
    "        #并没有进行具体的神经网络的操作\n",
    "\n",
    "    #之前操作的时候，这里输入的为([1,1,32,32])\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        print('forward = ***')\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        #self.conv1(x)为(1,6,3),所以需要6个卷积核进行操作，\n",
    "        # If the size is a square you can only specify a single number\n",
    "        #原始的数据为1*32*32的对应矩阵，使用3*3卷积核进行放缩\n",
    "        #32-3+1=28，28/2=14，6*14*14\n",
    "        #详情可以看这篇https://www.jianshu.com/p/45a26d278473\n",
    "        print(x.size())\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        #self.conv1(x)为(6,16,3),操作完之后x\n",
    "        #使用14-3+1=12，再经过一个max_pool2d之后为12/2=6，\n",
    "        #由于使用了16个对应的卷积核，所以结果为6*6*16\n",
    "        print(x.size())\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        #将输出的6*6*16铺平之后结果为576，这里self.num_flat_features(x)\n",
    "        #输出的对应结果为576，是所有值相乘\n",
    "        print(x.size())\n",
    "        #xprint('change x = ***')\n",
    "        #如果是torch.view(-1),则原向量会变成一维的结构\n",
    "        #t = torch.tensor([-0.3,-0.6,0.7,0.4,2.3,0.15])\n",
    "        #result = t.view(3,2)\n",
    "        #此时result变成([[-0.3 -0.6],\n",
    "        #               [0.7,0.4],\n",
    "        #               [2.3,0.15]])\n",
    "        ##view函数将张量x变形成一维的向量形式，\n",
    "        #总特征数并不改变，为接下来的全连接作准备\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #变换结果为[120,576]*[576,1]=[120]\n",
    "        print(x.size())\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #变换结果为[84,120],[84]\n",
    "        print(x.size())\n",
    "        x = self.fc3(x)\n",
    "        #变换结果为[10,84],[84]\n",
    "        print(x.size())\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        print('num_flat_features')\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        print('size = ***')\n",
    "        print(size)\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        print('num_features = ***')\n",
    "        print(num_features)\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "#这里将神经网络的结构打印出来，\n",
    "#从而得到相应的每一层的结构\n",
    "\n",
    "#stride中x表示卷积核的水平滑动步长\n",
    "#y表示卷积核的垂直滑动步长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params = ***\n",
      "torch.Size([6, 1, 3, 3])\n",
      "###\n",
      "torch.Size([6])\n",
      "###\n",
      "torch.Size([16, 6, 3, 3])\n",
      "###\n",
      "torch.Size([16])\n",
      "###\n",
      "torch.Size([120, 576])\n",
      "###\n",
      "torch.Size([120])\n",
      "###\n",
      "torch.Size([84, 120])\n",
      "###\n",
      "torch.Size([84])\n",
      "###\n",
      "torch.Size([10, 84])\n",
      "###\n",
      "torch.Size([10])\n",
      "###\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "#定义了对应的forward前向传播函数，以及backward后向传播函数\n",
    "params = list(net.parameters())\n",
    "#学习的参数由net.parameters()函数进行返回\n",
    "print('params = ***')\n",
    "for  i  in  range(0,len(params)):\n",
    "    print(params[i].size())\n",
    "    print('###')\n",
    "print('***')\n",
    "#这里面的params没看明白里面包含的参数内容\n",
    "#因为第一层为Conv2d(1,6,kernel_size=(3,3)),所以\n",
    "#第一层中共有6个对应的3*3矩阵\n",
    "#第二层为Conv2d(6,16,kernel_size=(3,3)),所以\n",
    "#第二层中共有6*16=96个对应的3*3矩阵\n",
    "#然后由于由in_features中的576转到out_features中对应的120\n",
    "#所以这里面需要乘上一个576*120的对应的矩阵\n",
    "#然后由于由in_features中的120转到out_features中对应的84\n",
    "#所以这里面需要乘上一个120*84的对应的矩阵\n",
    "#然后由于由in_features中的84转到out_features中对应的10\n",
    "#所以这里面需要乘上一个84*10的对应的矩阵\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对应的参数如下:\n",
    "params = ***\n",
    "torch.Size([6, 1, 3, 3])\n",
    "torch.Size([6])\n",
    "torch.Size([16, 6, 3, 3])\n",
    "torch.Size([16])\n",
    "torch.Size([120, 576])\n",
    "torch.Size([120])\n",
    "torch.Size([84, 120])\n",
    "torch.Size([84])\n",
    "torch.Size([10, 84])\n",
    "torch.Size([10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward = ***\n",
      "torch.Size([1, 6, 15, 15])\n",
      "torch.Size([1, 16, 6, 6])\n",
      "num_flat_features\n",
      "size = ***\n",
      "torch.Size([16, 6, 6])\n",
      "num_features = ***\n",
      "576\n",
      "torch.Size([1, 576])\n",
      "torch.Size([1, 120])\n",
      "torch.Size([1, 84])\n",
      "torch.Size([1, 10])\n",
      "tensor([[-0.0914,  0.0550,  0.0276,  0.0287, -0.1046, -0.1136,  0.1201, -0.0392,\n",
      "          0.0405, -0.0511]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn((1,1,32,32),requires_grad = True)\n",
    "#前面的input中加上对应的属性requires_grad = True的时候，\n",
    "#反向传播才能看到input的最终值\n",
    "out = net(input)\n",
    "print(out)\n",
    "#前面为w=[10,84],乘上对应的矩阵[1,84],结果为10,注意前面乘上的\n",
    "#w权重矩阵放在前面!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before backward\n",
      "tensor([[-0.0914,  0.0550,  0.0276,  0.0287, -0.1046, -0.1136,  0.1201, -0.0392,\n",
      "          0.0405, -0.0511]], grad_fn=<AddmmBackward>)\n",
      "after backward\n",
      "tensor([[[[-0.0002, -0.0003,  0.0005,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0002, -0.0008, -0.0002,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0003,  0.0010, -0.0014,  ...,  0.0026,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0003, -0.0003, -0.0011,  ...,  0.0010,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "#backward可能会积累梯度剩下，所以你需要提前进行\n",
    "#zero_grad()对它进行操作\n",
    "print('before backward')\n",
    "print(out)\n",
    "#将所有的梯度化为零并且反向传播使用随机的梯度\n",
    "out.backward(torch.randn(1,10))\n",
    "#到目前为止，只对浮点型数值使用backward函数\n",
    "#默认的require_grad的值为False，如果一个require\n",
    "#_grad的值为True，那么所有依附它的require_grad\n",
    "#的对应值都为True\n",
    "print('after backward')\n",
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any of tensors are non-scalar and require gradient,then the Jacobian-vector product would be computed.\n",
    "如果tensors不为常量需要梯度，Jacobian-vector乘积需要进行计算\n",
    "关于Jacobian-vector可以阅读相应的博文https://zhuanlan.zhihu.com/p/65609544\n",
    "Tensor变量的requires_grad属性默认为False，若一个节点requires_grad被设置为True，那么所有依赖它的节点requires_grad都为True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward = ***\n",
      "torch.Size([1, 6, 15, 15])\n",
      "torch.Size([1, 16, 6, 6])\n",
      "num_flat_features\n",
      "size = ***\n",
      "torch.Size([16, 6, 6])\n",
      "num_features = ***\n",
      "576\n",
      "torch.Size([1, 576])\n",
      "torch.Size([1, 120])\n",
      "torch.Size([1, 84])\n",
      "torch.Size([1, 10])\n",
      "target = ***\n",
      "tensor([[-0.5896, -0.1096,  0.4998, -1.6665, -0.1585,  0.8958, -1.9492,  0.8551,\n",
      "          1.4342, -0.7292]])\n",
      "tensor(1.1878, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)\n",
    "target = target.view(1,-1)\n",
    "print('target = ***')\n",
    "print(target)\n",
    "criterion = nn.MSELoss()\n",
    "#MSELoss对应的为均方误差函数\n",
    "loss = criterion(output,target)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
