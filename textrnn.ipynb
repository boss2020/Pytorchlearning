{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自然语言处理中更常用的是RNN:Recurrent Neural Network,能够更好地表达上下文信息\n",
    "RNN是在自然语言处理中非常标配的一个网络，在序列标注/命名体识别/Seq2seq灯场景中都有应用\n",
    "RNN的对应公式:P(w1,w2...wm)=(连乘(i=1,m))P(wi|w1,...wi-1)\n",
    "这里的每个词对应的概率依赖于前面所有的词对应的概率，与"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2idx = ***\n",
      "{'coffee': 0, 'milk': 1, 'love': 2, 'hate': 3, 'dog': 4, 'i': 5, 'like': 6}\n",
      "idx2word = ***\n",
      "{0: 'coffee', 1: 'milk', 2: 'love', 3: 'hate', 4: 'dog', 5: 'i', 6: 'like'}\n",
      "make_data\n",
      "word = ***\n",
      "['i', 'like', 'dog']\n",
      "input_batch = ***\n",
      "[array([[0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1.]])]\n",
      "word = ***\n",
      "['i', 'love', 'coffee']\n",
      "input_batch = ***\n",
      "[array([[0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1.]]), array([[0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0.]])]\n",
      "word = ***\n",
      "['i', 'hate', 'milk']\n",
      "input_batch = ***\n",
      "[array([[0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1.]]), array([[0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0.]]), array([[0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0.]])]\n",
      "----------------------------------------------------------------------------\n",
      "input_batch = !!!\n",
      "[array([[0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1.]]), array([[0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0.]]), array([[0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0.]])]\n",
      "target_batch = !!!\n",
      "[4, 0, 1]\n",
      "############################################################################\n",
      "input_batch.shape = ***\n",
      "torch.Size([3, 2, 7])\n",
      "target_batch.shape = ***\n",
      "torch.Size([3])\n",
      "dataset 0 = ***\n",
      "(tensor([[0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.]]), tensor(4))\n",
      "----------------loader = --------------------------\n",
      "img = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0.]]])\n",
      "label = ***\n",
      "tensor([4, 1])\n",
      "img = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0.]]])\n",
      "label = ***\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "  code by Tae Hwan Jung(Jeff Jung) @graykode, modify by wmathor\n",
    "'''\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "sentences = [ \"i like dog\", \"i love coffee\", \"i hate milk\"]\n",
    "\n",
    "word_list = \" \".join(sentences).split()\n",
    "#word_list=['i','like','dog','i','love','coffee','i','hate','milk']\n",
    "vocab = list(set(word_list))\n",
    "#vocab=['hate', 'love', 'milk', 'like', 'dog', 'coffee', 'i']\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "#word2idx = {'hate': 0, 'love': 1, 'milk': 2, 'like': 3, 'dog': 4, 'coffee': 5, 'i': 6}\n",
    "#!!!注意!!!每次word2idx与对应的单词编码的结果可能会不一样\n",
    "print('word2idx = ***')\n",
    "print(word2idx)\n",
    "idx2word = {i: w for i, w in enumerate(vocab)}\n",
    "#idx2word = {0: 'hate', 1: 'love', 2: 'milk', 3: 'like', 4: 'dog', 5: 'coffee', 6: 'i'}\n",
    "print('idx2word = ***')\n",
    "print(idx2word)\n",
    "n_class = len(vocab)\n",
    "#n_class = 7,因为总共有对应的7个单词\n",
    "# TextRNN Parameter\n",
    "batch_size = 2\n",
    "n_step = 2 # number of cells(= number of Step)\n",
    "n_hidden = 5 # number of hidden units in one cell\n",
    "\n",
    "def make_data(sentences):\n",
    "    print('make_data')\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for sen in sentences:\n",
    "        #print('sen = ***')\n",
    "        #print(sen)\n",
    "        word = sen.split()\n",
    "        print('word = ***')\n",
    "        print(word)\n",
    "        input = [word2idx[n] for n in word[:-1]]\n",
    "        #print('input = ***')\n",
    "        #print(input)\n",
    "        target = word2idx[word[-1]]\n",
    "        #将前两个单词作为对应的input，最后一个单词\n",
    "        #作为目标对应的target的值，比如'i','like','dog'\n",
    "        #input=[6,3]为'i','like'对应的值，'dog'=[4]为\n",
    "        #target对应的值\n",
    "        #print('target = ***')\n",
    "        #print(target)\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        #这里面n_class的对应值为7，numpy.eye()为生成对角矩阵\n",
    "        #对应的n_class值为7的时候为长度为7的对角矩阵，这里的\n",
    "        #input选择哪个的时候对应的值就为哪个，所以本质上是\n",
    "        #将对应的input的值进行one-hot的编码\n",
    "        print('input_batch = ***')\n",
    "        print(input_batch)\n",
    "        target_batch.append(target)\n",
    "    return input_batch, target_batch\n",
    "\n",
    "input_batch, target_batch = make_data(sentences)\n",
    "print('----------------------------------------------------------------------------')\n",
    "print('input_batch = !!!')\n",
    "print(input_batch)\n",
    "print('target_batch = !!!')\n",
    "print(target_batch)\n",
    "input_batch, target_batch = torch.Tensor(input_batch), torch.LongTensor(target_batch)\n",
    "print('############################################################################')\n",
    "print('input_batch.shape = ***')\n",
    "print(input_batch.shape)\n",
    "print('target_batch.shape = ***')\n",
    "print(target_batch.shape)\n",
    "'''\n",
    "input_batch为one-hot对应的向量组成的相应的数组\n",
    "target_batch为对应target的值\n",
    "'''\n",
    "dataset = Data.TensorDataset(input_batch, target_batch)\n",
    "#TensorDataset:对给定的tensor数据(样本和标签),将它们包装成dataset\n",
    "#注意如果是numpy的array，或者Pandas的DataFrame需要先转换成Tensor\n",
    "\n",
    "#将对应的tensor组装成一个input_batch和target_batch构成的一个整体\n",
    "print('dataset 0 = ***')\n",
    "print(dataset[0])\n",
    "#比如dataset[0]组成的对应的值为\n",
    "#(tensor([[0., 0., 0., 0., 0., 1., 0.],\n",
    "#        [0., 1., 0., 0., 0., 0., 0.]]), tensor(6))\n",
    "loader = Data.DataLoader(dataset, batch_size, True)\n",
    "print('----------------loader = --------------------------')\n",
    "for  img,label  in  loader:\n",
    "    print('img = ***')\n",
    "    print(img)\n",
    "    print('label = ***')\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size的对应值为2，\n",
    "dataset为刚才写的input组成的one-hot编码以及target_batch的tensor组成的\n",
    "对应的结果值\n",
    "DataLoader本质上是一个iterable,与python内置的list类似，并利用多进程\n",
    "来加速batch data，使用yield使用有限的内存\n",
    "\n",
    "DataLoader是一个高效、简洁、直观的网络输入数据结果，\n",
    "使用pytorch将数据加载到模型一般的操作顺序如下：\n",
    "1.创建一个Dataset对象\n",
    "2.创建一个DataLoader对象\n",
    "3.循环这个DataLoader对象，将img，label加载到模型中进行训练\n",
    "\n",
    "比如本例子之中对应的word2idx的对应值如下:\n",
    "{'hate':0,'like':1,'coffee':2,'milk':3,'love':4,'i':5,'dog':6}\n",
    "那么此时对应的dataset的内容为\n",
    "[([5,1],[6]),([5,4],[2]),([5,0],[3])]\n",
    "经过DataLoader之后，对应的loader的内容为\n",
    "[([5,4],[5,0]),([2],[3]),([5,1]),([6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__ TextRnn\n",
      "n_class = 7\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "epoch = 0\n",
      "x = ***\n",
      "torch.Size([2, 2, 7])\n",
      "y = ***\n",
      "torch.Size([2])\n",
      "n_hidden = ***\n",
      "5\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "x.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "forward = ***\n",
      "before transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0.]]])\n",
      "type X = ***\n",
      "<class 'torch.Tensor'>\n",
      "after transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0., 0., 0., 0.]]])\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "self.rnn = ***\n",
      "!!!out.shape = !!!\n",
      "torch.Size([2, 2, 5])\n",
      "!!!hidden.shape = !!!\n",
      "torch.Size([1, 2, 5])\n",
      "out = ###\n",
      "tensor([[ 0.0534, -0.4457, -0.1426, -0.0277,  0.0406],\n",
      "        [ 0.5495, -0.3867, -0.4052,  0.1321,  0.1161]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "out.shape = ###\n",
      "torch.Size([2, 5])\n",
      "n_hidden = 5,n_class = 7\n",
      "model = ###\n",
      "torch.Size([2, 7])\n",
      "pred = ***\n",
      "torch.Size([2, 7])\n",
      "y = ***\n",
      "torch.Size([2])\n",
      "pred = ###\n",
      "tensor([[-0.3092,  0.2039,  0.3116,  0.0104, -0.1216,  0.1047,  0.2810],\n",
      "        [-0.5412,  0.1235,  0.4823,  0.0342, -0.2314, -0.0737,  0.3901]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "y = ###\n",
      "tensor([4, 0])\n",
      "!!!before loss!!!\n",
      "loss = 2.361151\n",
      "loss = ***\n",
      "tensor(2.3612, grad_fn=<NllLossBackward>)\n",
      "x = ***\n",
      "torch.Size([1, 2, 7])\n",
      "y = ***\n",
      "torch.Size([1])\n",
      "n_hidden = ***\n",
      "5\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.]]])\n",
      "x.shape = ***\n",
      "torch.Size([1, 2, 7])\n",
      "forward = ***\n",
      "before transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0.]]])\n",
      "type X = ***\n",
      "<class 'torch.Tensor'>\n",
      "after transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 0., 0., 0.]]])\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.]]])\n",
      "self.rnn = ***\n",
      "!!!out.shape = !!!\n",
      "torch.Size([2, 1, 5])\n",
      "!!!hidden.shape = !!!\n",
      "torch.Size([1, 1, 5])\n",
      "out = ###\n",
      "tensor([[ 0.3064, -0.3806,  0.1280, -0.0378,  0.3097]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "out.shape = ###\n",
      "torch.Size([1, 5])\n",
      "n_hidden = 5,n_class = 7\n",
      "model = ###\n",
      "torch.Size([1, 7])\n",
      "pred = ***\n",
      "torch.Size([1, 7])\n",
      "y = ***\n",
      "torch.Size([1])\n",
      "pred = ###\n",
      "tensor([[-0.4457,  0.2604,  0.3168,  0.0022, -0.0261,  0.0303,  0.4619]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "y = ###\n",
      "tensor([1])\n",
      "!!!before loss!!!\n",
      "loss = 1.807101\n",
      "loss = ***\n",
      "tensor(1.8071, grad_fn=<NllLossBackward>)\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "epoch = 1\n",
      "x = ***\n",
      "torch.Size([2, 2, 7])\n",
      "y = ***\n",
      "torch.Size([2])\n",
      "n_hidden = ***\n",
      "5\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "x.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "forward = ***\n",
      "before transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1.]]])\n",
      "type X = ***\n",
      "<class 'torch.Tensor'>\n",
      "after transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1.]]])\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "self.rnn = ***\n",
      "!!!out.shape = !!!\n",
      "torch.Size([2, 2, 5])\n",
      "!!!hidden.shape = !!!\n",
      "torch.Size([1, 2, 5])\n",
      "out = ###\n",
      "tensor([[ 0.5450, -0.3830, -0.3940,  0.1422,  0.1070],\n",
      "        [ 0.0471, -0.4396, -0.1294, -0.0174,  0.0347]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "out.shape = ###\n",
      "torch.Size([2, 5])\n",
      "n_hidden = 5,n_class = 7\n",
      "model = ###\n",
      "torch.Size([2, 7])\n",
      "pred = ***\n",
      "torch.Size([2, 7])\n",
      "y = ***\n",
      "torch.Size([2])\n",
      "pred = ###\n",
      "tensor([[-0.5313,  0.1285,  0.4724,  0.0259, -0.2219, -0.0770,  0.3814],\n",
      "        [-0.3013,  0.2093,  0.3022,  0.0047, -0.1098,  0.1027,  0.2741]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "y = ###\n",
      "tensor([0, 4])\n",
      "!!!before loss!!!\n",
      "loss = 2.348403\n",
      "loss = ***\n",
      "tensor(2.3484, grad_fn=<NllLossBackward>)\n",
      "x = ***\n",
      "torch.Size([1, 2, 7])\n",
      "y = ***\n",
      "torch.Size([1])\n",
      "n_hidden = ***\n",
      "5\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.]]])\n",
      "x.shape = ***\n",
      "torch.Size([1, 2, 7])\n",
      "forward = ***\n",
      "before transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0.]]])\n",
      "type X = ***\n",
      "<class 'torch.Tensor'>\n",
      "after transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 0., 0., 0.]]])\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.]]])\n",
      "self.rnn = ***\n",
      "!!!out.shape = !!!\n",
      "torch.Size([2, 1, 5])\n",
      "!!!hidden.shape = !!!\n",
      "torch.Size([1, 1, 5])\n",
      "out = ###\n",
      "tensor([[ 0.3053, -0.3793,  0.1410, -0.0283,  0.3016]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "out.shape = ###\n",
      "torch.Size([1, 5])\n",
      "n_hidden = 5,n_class = 7\n",
      "model = ###\n",
      "torch.Size([1, 7])\n",
      "pred = ***\n",
      "torch.Size([1, 7])\n",
      "y = ***\n",
      "torch.Size([1])\n",
      "pred = ###\n",
      "tensor([[-0.4385,  0.2708,  0.3086, -0.0058, -0.0179,  0.0276,  0.4565]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "y = ###\n",
      "tensor([1])\n",
      "!!!before loss!!!\n",
      "loss = 1.796150\n",
      "loss = ***\n",
      "tensor(1.7961, grad_fn=<NllLossBackward>)\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "epoch = 2\n",
      "x = ***\n",
      "torch.Size([2, 2, 7])\n",
      "y = ***\n",
      "torch.Size([2])\n",
      "n_hidden = ***\n",
      "5\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "x.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "forward = ***\n",
      "before transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1.]]])\n",
      "type X = ***\n",
      "<class 'torch.Tensor'>\n",
      "after transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1.]]])\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "self.rnn = ***\n",
      "!!!out.shape = !!!\n",
      "torch.Size([2, 2, 5])\n",
      "!!!hidden.shape = !!!\n",
      "torch.Size([1, 2, 5])\n",
      "out = ###\n",
      "tensor([[ 0.3055, -0.3794,  0.1477, -0.0235,  0.2973],\n",
      "        [ 0.0431, -0.4361, -0.1163, -0.0077,  0.0285]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "out.shape = ###\n",
      "torch.Size([2, 5])\n",
      "n_hidden = 5,n_class = 7\n",
      "model = ###\n",
      "torch.Size([2, 7])\n",
      "pred = ***\n",
      "torch.Size([2, 7])\n",
      "y = ***\n",
      "torch.Size([2])\n",
      "pred = ###\n",
      "tensor([[-0.4351,  0.2766,  0.3046, -0.0101, -0.0143,  0.0262,  0.4541],\n",
      "        [-0.2942,  0.2176,  0.2936, -0.0015, -0.0994,  0.1007,  0.2687]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "y = ###\n",
      "tensor([1, 4])\n",
      "!!!before loss!!!\n",
      "loss = 1.961994\n",
      "loss = ***\n",
      "tensor(1.9620, grad_fn=<NllLossBackward>)\n",
      "x = ***\n",
      "torch.Size([1, 2, 7])\n",
      "y = ***\n",
      "torch.Size([1])\n",
      "n_hidden = ***\n",
      "5\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.]]])\n",
      "x.shape = ***\n",
      "torch.Size([1, 2, 7])\n",
      "forward = ***\n",
      "before transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0.]]])\n",
      "type X = ***\n",
      "<class 'torch.Tensor'>\n",
      "after transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0., 0., 0., 0.]]])\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.]]])\n",
      "self.rnn = ***\n",
      "!!!out.shape = !!!\n",
      "torch.Size([2, 1, 5])\n",
      "!!!hidden.shape = !!!\n",
      "torch.Size([1, 1, 5])\n",
      "out = ###\n",
      "tensor([[ 0.5415, -0.3814, -0.3768,  0.1563,  0.0937]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "out.shape = ###\n",
      "torch.Size([1, 5])\n",
      "n_hidden = 5,n_class = 7\n",
      "model = ###\n",
      "torch.Size([1, 7])\n",
      "pred = ***\n",
      "torch.Size([1, 7])\n",
      "y = ***\n",
      "torch.Size([1])\n",
      "pred = ###\n",
      "tensor([[-0.5187,  0.1416,  0.4590,  0.0133, -0.2095, -0.0815,  0.3713]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "y = ###\n",
      "tensor([0])\n",
      "!!!before loss!!!\n",
      "loss = 2.536971\n",
      "loss = ***\n",
      "tensor(2.5370, grad_fn=<NllLossBackward>)\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "epoch = 3\n",
      "x = ***\n",
      "torch.Size([2, 2, 7])\n",
      "y = ***\n",
      "torch.Size([2])\n",
      "n_hidden = ***\n",
      "5\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "x.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "forward = ***\n",
      "before transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0.]]])\n",
      "type X = ***\n",
      "<class 'torch.Tensor'>\n",
      "after transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0.]]])\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "self.rnn = ***\n",
      "!!!out.shape = !!!\n",
      "torch.Size([2, 2, 5])\n",
      "!!!hidden.shape = !!!\n",
      "torch.Size([1, 2, 5])\n",
      "out = ###\n",
      "tensor([[ 0.3050, -0.3787,  0.1604, -0.0142,  0.2893],\n",
      "        [ 0.5399, -0.3810, -0.3716,  0.1607,  0.0893]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "out.shape = ###\n",
      "torch.Size([2, 5])\n",
      "n_hidden = 5,n_class = 7\n",
      "model = ###\n",
      "torch.Size([2, 7])\n",
      "pred = ***\n",
      "torch.Size([2, 7])\n",
      "y = ***\n",
      "torch.Size([2])\n",
      "pred = ###\n",
      "tensor([[-0.4287,  0.2873,  0.2966, -0.0181, -0.0066,  0.0234,  0.4490],\n",
      "        [-0.5143,  0.1456,  0.4546,  0.0092, -0.2061, -0.0830,  0.3678]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "y = ###\n",
      "tensor([1, 0])\n",
      "!!!before loss!!!\n",
      "loss = 2.155291\n",
      "loss = ***\n",
      "tensor(2.1553, grad_fn=<NllLossBackward>)\n",
      "x = ***\n",
      "torch.Size([1, 2, 7])\n",
      "y = ***\n",
      "torch.Size([1])\n",
      "n_hidden = ***\n",
      "5\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.]]])\n",
      "x.shape = ***\n",
      "torch.Size([1, 2, 7])\n",
      "forward = ***\n",
      "before transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1.]]])\n",
      "type X = ***\n",
      "<class 'torch.Tensor'>\n",
      "after transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 1.]]])\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.]]])\n",
      "self.rnn = ***\n",
      "!!!out.shape = !!!\n",
      "torch.Size([2, 1, 5])\n",
      "!!!hidden.shape = !!!\n",
      "torch.Size([1, 1, 5])\n",
      "out = ###\n",
      "tensor([[ 0.0378, -0.4321, -0.0975,  0.0058,  0.0193]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "out.shape = ###\n",
      "torch.Size([1, 5])\n",
      "n_hidden = 5,n_class = 7\n",
      "model = ###\n",
      "torch.Size([1, 7])\n",
      "pred = ***\n",
      "torch.Size([1, 7])\n",
      "y = ***\n",
      "torch.Size([1])\n",
      "pred = ###\n",
      "tensor([[-0.2843,  0.2301,  0.2813, -0.0107, -0.0855,  0.0979,  0.2611]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "y = ###\n",
      "tensor([4])\n",
      "!!!before loss!!!\n",
      "loss = 2.119527\n",
      "loss = ***\n",
      "tensor(2.1195, grad_fn=<NllLossBackward>)\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "epoch = 4\n",
      "x = ***\n",
      "torch.Size([2, 2, 7])\n",
      "y = ***\n",
      "torch.Size([2])\n",
      "n_hidden = ***\n",
      "5\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "x.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "forward = ***\n",
      "before transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0.]]])\n",
      "type X = ***\n",
      "<class 'torch.Tensor'>\n",
      "after transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1., 0., 0., 0.]]])\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "self.rnn = ***\n",
      "!!!out.shape = !!!\n",
      "torch.Size([2, 2, 5])\n",
      "!!!hidden.shape = !!!\n",
      "torch.Size([1, 2, 5])\n",
      "out = ###\n",
      "tensor([[ 0.0352, -0.4298, -0.0914,  0.0105,  0.0169],\n",
      "        [ 0.3034, -0.3775,  0.1722, -0.0053,  0.2817]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "out.shape = ###\n",
      "torch.Size([2, 5])\n",
      "n_hidden = 5,n_class = 7\n",
      "model = ###\n",
      "torch.Size([2, 7])\n",
      "pred = ***\n",
      "torch.Size([2, 7])\n",
      "y = ***\n",
      "torch.Size([2])\n",
      "pred = ###\n",
      "tensor([[-0.2811,  0.2336,  0.2769, -0.0133, -0.0804,  0.0969,  0.2583],\n",
      "        [-0.4217,  0.2969,  0.2887, -0.0256,  0.0007,  0.0209,  0.4437]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "y = ###\n",
      "tensor([4, 1])\n",
      "!!!before loss!!!\n",
      "loss = 1.941550\n",
      "loss = ***\n",
      "tensor(1.9415, grad_fn=<NllLossBackward>)\n",
      "x = ***\n",
      "torch.Size([1, 2, 7])\n",
      "y = ***\n",
      "torch.Size([1])\n",
      "n_hidden = ***\n",
      "5\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.]]])\n",
      "x.shape = ***\n",
      "torch.Size([1, 2, 7])\n",
      "forward = ***\n",
      "before transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0.]]])\n",
      "type X = ***\n",
      "<class 'torch.Tensor'>\n",
      "after transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0., 0., 0., 0.]]])\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.]]])\n",
      "self.rnn = ***\n",
      "!!!out.shape = !!!\n",
      "torch.Size([2, 1, 5])\n",
      "!!!hidden.shape = !!!\n",
      "torch.Size([1, 1, 5])\n",
      "out = ###\n",
      "tensor([[ 0.5351, -0.3785, -0.3555,  0.1740,  0.0774]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "out.shape = ###\n",
      "torch.Size([1, 5])\n",
      "n_hidden = 5,n_class = 7\n",
      "model = ###\n",
      "torch.Size([1, 7])\n",
      "pred = ***\n",
      "torch.Size([1, 7])\n",
      "y = ***\n",
      "torch.Size([1])\n",
      "pred = ###\n",
      "tensor([[-0.5019,  0.1572,  0.4412, -0.0025, -0.1945, -0.0875,  0.3574]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "y = ###\n",
      "tensor([0])\n",
      "!!!before loss!!!\n",
      "loss = 2.516379\n",
      "loss = ***\n",
      "tensor(2.5164, grad_fn=<NllLossBackward>)\n",
      "input = ***\n",
      "[['i', 'like'], ['i', 'love'], ['i', 'hate']]\n",
      "hidden = ***\n",
      "torch.Size([1, 3, 5])\n",
      "forward = ***\n",
      "before transpose\n",
      "X = ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0.]]])\n",
      "type X = ***\n",
      "<class 'torch.Tensor'>\n",
      "after transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0.]]])\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "self.rnn = ***\n",
      "!!!out.shape = !!!\n",
      "torch.Size([2, 3, 5])\n",
      "!!!hidden.shape = !!!\n",
      "torch.Size([1, 3, 5])\n",
      "out = ###\n",
      "tensor([[ 0.0300, -0.4252, -0.0792,  0.0198,  0.0121],\n",
      "        [ 0.5330, -0.3775, -0.3504,  0.1783,  0.0735],\n",
      "        [ 0.3012, -0.3750,  0.1839,  0.0039,  0.2747]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "out.shape = ###\n",
      "torch.Size([3, 5])\n",
      "n_hidden = 5,n_class = 7\n",
      "model = ###\n",
      "torch.Size([3, 7])\n",
      "predict1 = ***\n",
      "tensor([[-0.2749,  0.2406,  0.2683, -0.0184, -0.0702,  0.0949,  0.2528],\n",
      "        [-0.4976,  0.1605,  0.4367, -0.0062, -0.1907, -0.0890,  0.3537],\n",
      "        [-0.4153,  0.3058,  0.2807, -0.0324,  0.0089,  0.0183,  0.4381]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "predict2 = ***\n",
      "torch.return_types.max(\n",
      "values=tensor([[0.2683],\n",
      "        [0.4367],\n",
      "        [0.4381]]),\n",
      "indices=tensor([[2],\n",
      "        [2],\n",
      "        [6]]))\n",
      "predict = ***\n",
      "tensor([[2],\n",
      "        [2],\n",
      "        [6]])\n",
      "[['i', 'like'], ['i', 'love'], ['i', 'hate']] -> ['love', 'love', 'like']\n"
     ]
    }
   ],
   "source": [
    "class TextRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextRNN, self).__init__()\n",
    "        print('__init__ TextRnn')\n",
    "        print('n_class = %d'%n_class)\n",
    "        self.rnn = nn.RNN(input_size=n_class, hidden_size=n_hidden)\n",
    "        #input_size:输入特征的维度，hidden_size:隐藏层神经元的个数\n",
    "        #这里体现了使用RNN对文本进行操作分类\n",
    "        self.fc = nn.Linear(n_hidden, n_class)\n",
    "        #乘上对应的n_hidden*n_class的对应的矩阵\n",
    "\n",
    "    def forward(self, hidden, X):\n",
    "        print('forward = ***')\n",
    "        # X: [batch_size, n_step, n_class]\n",
    "        print('before transpose')\n",
    "        print('X = ***')\n",
    "        print(X)\n",
    "        print('type X = ***')\n",
    "        print(type(X))\n",
    "        #变换之前X的对应的矩阵为\n",
    "        #[[[0,0,0,0,1,0,0],\n",
    "        #  [0,0,0,0,0,0,1],\n",
    "        #  [0,0,0,0,1,0,0],\n",
    "        #  [0,0,0,1,0,0,0]]]\n",
    "        X = X.transpose(0, 1) # X : [n_step, batch_size, n_class]\n",
    "        #pytorch中的transpose方法的作用是交换矩阵的两个维度\n",
    "        \n",
    "        #transpose中只有两个参数，torch.transpose中有三个参数\n",
    "        print('after transpose')\n",
    "        print('X = ***')\n",
    "        print(X)\n",
    "        #变换之后X对应的矩阵为\n",
    "        #[[[0,0,0,0,1,0,0],\n",
    "        #  [0,0,0,0,1,0,0],\n",
    "        #  [0,0,0,0,0,0,1],\n",
    "        #  [0,0,0,1,0,0,0]]]\n",
    "        print('hidden = ***')\n",
    "        print(hidden)\n",
    "        print('self.rnn = ***')\n",
    "        out, hidden = self.rnn(X, hidden)\n",
    "        #对应的self.rnn,这里使用nn.rnn(input_size,hidden_size)\n",
    "        #其中input_size为输入特征的维度，hidden_size为隐藏层神经元的\n",
    "        #对应的个数，或者也叫输出的维度\n",
    "        \n",
    "        #比如这里定义了rnn_layer = nn.RNN(input_size=1027,hidden_size=256)\n",
    "        #然后这里面使用self.runn(X,5)(隐藏层对应的个数为5)\n",
    "        #输入的X为对应的([35,2,1027])的对应的矩阵，输出的Y为对应的\n",
    "        #[35,2,256]的对应的矩阵，\n",
    "        \n",
    "        # out : [n_step, batch_size, num_directions(=1) * n_hidden]\n",
    "        # hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "        print('!!!out.shape = !!!')\n",
    "        print(out.shape)\n",
    "        print('!!!hidden.shape = !!!')\n",
    "        print(hidden.shape)\n",
    "        #原先的out对应的矩阵内容为\n",
    "        #out = tensor([[[-0.3484, -0.1260, -0.2226, -0.0077, -0.4533],\n",
    "        #[-0.3484, -0.1260, -0.2226, -0.0077, -0.4533]],\n",
    "        #[[-0.5938,  0.3629, -0.2776,  0.2037, -0.1790],\n",
    "        #[-0.7574, -0.4174, -0.5710,  0.1534,  0.0740]]],grad_fn=<SelectBackward>)\n",
    "        \n",
    "        out = out[-1] # [batch_size, num_directions(=1) * n_hidden] ⭐\n",
    "        #经过out[-1]的结果之后，对应的内容为\n",
    "        #out = tensor([[[-0.5938,  0.3629, -0.2776,  0.2037, -0.1790],\n",
    "        #[-0.7574, -0.4174, -0.5710,  0.1534,  0.0740]]],grad_fn=<SelectBackward>)\n",
    "        #因为总共数组里存在两个对应的内容，所以[-1]选取的是最后面的那个\n",
    "        print('out = ###')\n",
    "        print(out)\n",
    "        print('out.shape = ###')\n",
    "        print(out.shape)\n",
    "        model = self.fc(out)\n",
    "        #这里的定义为self.fc = nn.Linear(n_hidden,n_class)\n",
    "        print('n_hidden = %d,n_class = %d'%(n_hidden,n_class))\n",
    "        print('model = ###')\n",
    "        print(model.shape)\n",
    "        #这里面的内容为n_hidden = 5,n_class = 7,本身输入的内容为\n",
    "        #2*5的对应的内容，所以输出的内容为2*7的对应的内容，因为总共有\n",
    "        #7个类别，需要对应到7个类别\n",
    "        return model\n",
    "\n",
    "model = TextRNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#分类问题的处理一般都使用交叉熵损失函数\n",
    "#对应的公式为H(x)=-(i=1~n的和)P(xi)log(P(xi))\n",
    "#相对熵(KL散度)概念:DKL(p||q) = (i=1~n)p(xi)log(p(xi)/q(xi)),\n",
    "#其中这里的q(xi)代表的含义为原先预测的对应的概率\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "for epoch in range(5):\n",
    "    print('--------------------------------------------------------------------------------------------------------')\n",
    "    print('epoch = %d'%epoch)\n",
    "    for x, y in loader:\n",
    "        print('x = ***')\n",
    "        print(x.shape)\n",
    "        print('y = ***')\n",
    "        print(y.shape)\n",
    "        # hidden : [num_layers * num_directions, batch, hidden_size]\n",
    "        print('n_hidden = ***')\n",
    "        print(n_hidden)\n",
    "        hidden = torch.zeros(1, x.shape[0], n_hidden)\n",
    "        # x : [batch_size, n_step, n_class]\n",
    "        #torch.zeros(*size,*,out=None,dtype=None,layout=torch.strided,\n",
    "        #device=None,requires_grad=False)\n",
    "        #这里的out(Tensor)代表着对应的the  output tensor,\n",
    "        #x.shape[0] = 2,n_hidden = 5,对应的矩阵为1*2*5\n",
    "        #!!!rnn当中的隐藏层的对应输入必须为零矩阵!!!\n",
    "        \n",
    "        #实际形成的是一个1*2*5对应的相应的矩阵\n",
    "        print('hidden = ***')\n",
    "        print(hidden)\n",
    "        #hidden = 5,为上面的对应的隐藏单元\n",
    "        \n",
    "        #这里面调用model里面对应的forward()的相应的函数\n",
    "        print('x.shape = ***')\n",
    "        print(x.shape)\n",
    "        pred = model(hidden, x)\n",
    "        #之前的model = TextRNN()只是定义了相应的参数，\n",
    "        #使用pred = model(hidden,x)直接进入相应的forward函数之中进行运行\n",
    "        print('pred = ***')\n",
    "        print(pred.shape)\n",
    "        print('y = ***')\n",
    "        print(y.shape)\n",
    "        # pred : [batch_size, n_class], y : [batch_size] (LongTensor, not one-hot)\n",
    "        # pred为输出的对应的2*7的矩阵\n",
    "        print('pred = ###')\n",
    "        print(pred)\n",
    "        print('y = ###')\n",
    "        print(y)\n",
    "        print('!!!before loss!!!')\n",
    "        loss = criterion(pred, y)\n",
    "        print('loss = %.6f'%loss)\n",
    "        #这里在求criterion的时候，pred=(2,7),y=(2)\n",
    "        #前面定义了criterion = nn.CrossEntropyLoss()\n",
    "        #维度前面的维度可以不同，后面的维度必须相同\n",
    "        \n",
    "        #所以这里为pred与y的交叉熵损失函数\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "        optimizer.zero_grad()\n",
    "        print('loss = ***')\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "input = [sen.split()[:2] for sen in sentences]\n",
    "#input为截取的前面的两个单词作为相应的输入的内容\n",
    "# Predict,!!!\n",
    "print('input = ***')\n",
    "print(input)\n",
    "hidden = torch.zeros(1, len(input), n_hidden)\n",
    "print('hidden = ***')\n",
    "print(hidden.shape)\n",
    "#hidden对应的torch的大小为([1,3,5])\n",
    "\n",
    "#这里面的model相当于前面训练过的对应的model的内容\n",
    "predict1 = model(hidden, input_batch)\n",
    "print('predict1 = ***')\n",
    "print(predict1)\n",
    "#得到对应的不同类别的预测的概率值\n",
    "predict2 = predict1.data.max(1,keepdim=True)\n",
    "#提取出着所有单词当中出现最大的概率，提取出来的\n",
    "#tensor有两个部分，第一个部分为value是对应的概率\n",
    "#第二个部分为indices为对应的概率相应的具体的数值\n",
    "print('predict2 = ***')\n",
    "print(predict2)\n",
    "predict = predict2[1]\n",
    "print('predict = ***')\n",
    "print(predict)\n",
    "#predict = model(hidden, input_batch).data.max(1, keepdim=True)[1]\n",
    "#对应的预测种类的结果\n",
    "print([sen.split()[:2] for sen in sentences], '->', [idx2word[n.item()] for n in predict.squeeze()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.CrossEntropyLoss??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `F.cross_entropy` not found.\n"
     ]
    }
   ],
   "source": [
    "F.cross_entropy??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "程序运行的过程解析：首先\n",
    "x = tensor([[[0., 1., 0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 1., 0., 0.]],\n",
    "\n",
    "        [[0., 1., 0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0., 1., 0.]]])\n",
    "(x为起始的两个对应的句子)\n",
    "y = tensor([2,0])\n",
    "(y为对应的两个答案内容的标签)\n",
    "\n",
    "接着对应的hidden矩阵的内容为\n",
    "hidden = tensor([[[0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer的step为什么不能放在min-batch的循环之外，optimizer.step和loss.backward的区别\n",
    "1.明确optimizer优化器的作用:根据网络反向传播的梯度信息更新网络的参数，以起到降低loss函数计算值的作用\n",
    "    1.优化器需要知道当前网络或者别的模型的参数空间\n",
    "    2.优化器需要知道反向传播的梯度信息\n",
    "    optimizer更新参数空间需要基于反向梯度，因此当调用optimizer.step()的时候应当是loss.backward()的时候，所以经常是loss.backward()后面跟上optimizer.step()函数\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rnn前向神经模型对应的网站https://blog.csdn.net/weixin_42792500/article/details/81254313\n",
    "可以看出隐藏层输入必须为零矩阵，所以"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
